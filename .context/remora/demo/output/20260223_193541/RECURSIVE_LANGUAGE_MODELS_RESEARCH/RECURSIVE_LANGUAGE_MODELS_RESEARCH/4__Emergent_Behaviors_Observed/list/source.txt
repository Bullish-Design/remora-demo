1.  **Grepping / Filtering via Priors:** Without being told to, models frequently used standard Python string manipulation or Regex to filter the 10M token context down to just a few hundred relevant lines before ever calling a sub-LM, drastically saving cost and avoiding context rot.
2.  **Peeking:** Models would frequently `print()` the first 2000 characters of a context to learn its formatting (e.g., realizing it's a CSV or Markdown) before deciding how to chunk it.
3.  **Partition + Map (Map-Reduce):** When semantic understanding was required across the whole document (e.g., the OOLONG tasks), models naturally chunked the data and wrote loops to execute sub-LLMs over each chunk, aggregating the results back into a Python list.
4.  **Long-Output Synthesis:** For tasks requiring massive text generation (e.g., generating pairs of users), standard LLMs run out of output tokens. RLMs solved this by storing the partial outputs of sub-LLMs in a massive REPL variable and returning `FINAL_VAR(my_huge_list)` to bypass the token generation limit entirely.
5.  **Programmatic Verification:** Models would occasionally use `llm_query` to verify the accuracy of a previous `llm_query` output, or write a dedicated Python script to run an assertion check on the extracted data.