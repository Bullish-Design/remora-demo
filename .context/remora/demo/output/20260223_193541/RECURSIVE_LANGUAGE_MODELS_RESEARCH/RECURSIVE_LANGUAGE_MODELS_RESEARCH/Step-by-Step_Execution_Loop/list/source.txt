1.  **Environment Initialization:** The massive input context (e.g., 1000 documents, or a 1M token codebase) is loaded into the memory of a Python REPL sandbox as a variable (e.g., `context`).
2.  **Root LM Prompting:** The Root LM is prompted with the user's query and the *metadata* of the context (e.g., "Your context is a list of strings with 8.3M characters"). It does *not* receive the text of the context.
3.  **Code as Navigation:** The Root LM writes a Python code block (tagged with ```repl) to interact with the environment.
    *   *Example (Grepping):* It might write `[line for line in context if "error" in line]` to filter the context based on its priors.
    *   *Example (Peeking):* It might write `print(context[:1000])` to understand the data structure.
4.  **Recursive Sub-Calling (The Core Engine):** The REPL environment exposes a function `llm_query(prompt)`. The Root LM uses Python loops or list comprehensions to chunk the massive context and fire off sub-LLM calls for each chunk.
    *   *Example (Map-Reduce):*
        ```python
        answers = []
        for doc in context:
            if "target_word" in doc:
                res = llm_query(f"Extract the specific date from this doc: {doc}")
                answers.append(res)
        ```
5.  **State Accumulation (Buffer Variables):** The outputs of the sub-LMs are stored in Python variables (e.g., `buffers = []`) within the REPL.
6.  **Termination:** Once the Root LM has programmatically synthesized the data in its buffers, it outputs a final answer wrapped in a specific tag (e.g., `FINAL(answer)` or `FINAL_VAR(variable_name)`).