The core thesis of an RLM is to stop feeding the massive context directly into the LLM's prompt. Instead, the context is offloaded into an external execution environment (a Python REPL) as a queryable variable. The LLM is then placed into this environment and tasked with writing code to programmatically explore, chunk, and recursively spawn sub-LLM calls over specific pieces of that context to build up a final answer.