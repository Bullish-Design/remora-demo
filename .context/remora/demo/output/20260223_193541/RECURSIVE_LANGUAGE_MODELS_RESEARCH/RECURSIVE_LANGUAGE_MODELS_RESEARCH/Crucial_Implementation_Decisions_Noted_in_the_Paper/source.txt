*   **Asymmetric Model Pairing is Cost-Effective:** To manage API costs, the paper strongly recommends using a highly capable model for the Root LM (e.g., GPT-5) to write the REPL code and manage the reasoning, but a much cheaper/smaller model for the Recursive Sub-LMs (e.g., GPT-5-mini) to handle the map-reduce extraction tasks.
*   **Blocking vs. Asynchronous Calls:** The paper admits their implementation used blocking (sequential) sub-LM calls, making tasks very slow. They explicitly call out asynchrony (running the 100 `llm_query` map operations in parallel) as a major required optimization for real-world systems.
*   **Prompt Specificity per Model:** The paper found that Qwen-Coder models tended to abuse the `llm_query` function, firing off thousands of sub-calls when simple regex would do. They had to modify the system prompt specifically for Qwen to warn against excessive sub-calling and encourage batching.