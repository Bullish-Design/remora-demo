A critical enabler of this architecture is the backend inference engine. Using `vLLM` with batched inference and dynamically loaded on-demand LoRA adapters allows this to scale efficiently.

* We can run 10+ "tiny" fine-tuned logic models concurrently.
* We will not swamp the VRAM and will not suffer severe model loading overhead.
* The system relies on continuous concurrent batched inference throughput, seamlessly swapping adapters as the swarm processes different nodes.