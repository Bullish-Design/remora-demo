services:
  tailscale:
    build:
      context: .
      dockerfile: Dockerfile.tailscale
    container_name: tailscale-vllm
    # This hostname becomes the Tailscale node name and the DNS name you use
    # to reach the server: http://remora-server:8000/v1
    hostname: ${TS_HOSTNAME:-remora-server}
    env_file:
      - .env
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY}
      - TS_STATE_DIR=${TS_STATE_DIR:-/var/lib/tailscale}
      # Enables `ssh root@remora-server` without managing SSH keys
      - TS_SSH=${TS_SSH:-true}
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
      # Allows this container to restart the vllm-server container via Docker CLI
      - /var/run/docker.sock:/var/run/docker.sock
      # Mounts the project directory so `git pull` works inside the container
      - .:/app
    cap_add:
      - net_admin
      - sys_module
    restart: unless-stopped

  agents-server:
    build:
      context: .
      dockerfile: Dockerfile.agents
    container_name: agents-server
    network_mode: service:tailscale
    depends_on:
      - tailscale
    env_file:
      - .env
    environment:
      - AGENTS_DIR=${AGENTS_DIR:-/app/agents}
    volumes:
      - ${AGENTS_DIR:-../agents}:/app/agents:ro
    restart: unless-stopped

  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: vllm-gemma
    # Shares the Tailscale container's network namespace.
    # The vLLM API appears on the Tailscale node at port 8000 — no port
    # forwarding through Windows required.
    network_mode: service:tailscale
    depends_on:
      - tailscale
    env_file:
      - .env
    environment:
      # Required if the model is gated on Hugging Face
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      # Redirect all downloads and cache to a mounted SSD, not the container layer
      - VLLM_CACHE_ROOT=${VLLM_CACHE_ROOT:-/models/cache}
      - HF_HOME=${HF_HOME:-/models/cache}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: ${VLLM_GPU_COUNT:-1}
              capabilities: [gpu]
    volumes:
      # Adjust these paths to match your actual SSD drive letters.
      # WSL2 exposes Windows drives under /mnt/ (D: drive → /mnt/d/)
      # Put the base model on your fastest NVMe; adapters can go on any SSD.
      - ${VLLM_BASE_MODEL_PATH}:/models/base
      - ${VLLM_ADAPTERS_PATH}:/models/adapters
      - ${VLLM_CACHE_PATH}:/models/cache
    # Prevents shared-memory exhaustion errors in vLLM
    ipc: host
    restart: unless-stopped

volumes:
  tailscale-data:
